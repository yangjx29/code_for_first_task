import sys
import os
sys.path.append('../../')
sys.path.append('../../python_parser')
sys.path.append('../code')
import json
import argparse
import warnings
import torch
from model import CodeBERT, GraphCodeBERT, CodeT5
from run import CodeBertTextDataset, GraphCodeBertTextDataset, CodeT5TextDataset
from utils import set_seed
from attacker import Attacker
from pso import PSOAttack
from transformers import (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer, RobertaForMaskedLM,T5Config, T5ForConditionalGeneration)
from tqdm import tqdm
import fasttext
import fasttext.util
import time
# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
# warnings.simplefilter(action='ignore', category=FutureWarning)
import logging
from codebleu import calc_codebleu

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)

MODEL_CLASSES = {
    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),
    'codet5': (T5Config, T5ForConditionalGeneration, RobertaTokenizer)
}


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument("--eval_data_file", default=None, type=str,
                        help="An optional input evaluation data file to evaluate the perplexity on (a text file).")
    parser.add_argument('--seed', type=int, default=123456,
                        help="random seed for initialization")
    parser.add_argument("--cache_dir", default="", type=str,
                        help="Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)")
    parser.add_argument("--attack_random", default=0, type=int,
                        help="attack random.")
    parser.add_argument("--model_name", default="", type=str,
                        help="model type.")

    args = parser.parse_args()
    args.device = torch.device("cuda")
    args.output_dir = '../code/saved_models'
    args.model_type = 'roberta'
    args.eval_batch_size = 64
    args.block_size = 512
    args.language_type = 'c'
    args.use_ga = True

    if args.model_name == 'codebert':
        args.tokenizer_name = 'microsoft/codebert-base-mlm'
        args.model_name_or_path = 'microsoft/codebert-base-mlm'
        args.base_model = 'microsoft/codebert-base-mlm'
    elif args.model_name == 'graphcodebert':
        args.code_length = 448
        args.data_flow_length = 64
        args.tokenizer_name = 'microsoft/graphcodebert-base'
        args.model_name_or_path = 'microsoft/graphcodebert-base'
        args.base_model = 'microsoft/graphcodebert-base'
    elif args.model_name == 'codet5':
        args.model_type = 'codet5'
        args.tokenizer_name = 'Salesforce/codet5-base-multi-sum'
        args.model_name_or_path = 'Salesforce/codet5-base-multi-sum'
        args.base_model = 'microsoft/codebert-base-mlm'

    set_seed(args.seed)
    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]
    config = config_class.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None)
    config.num_labels = 1
    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,
                                                do_lower_case=False,
                                                cache_dir=args.cache_dir if args.cache_dir else None)
    if args.block_size <= 0:
        args.block_size = tokenizer.max_len_single_sentence
    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)

    if args.model_name_or_path:
        model = model_class.from_pretrained(args.model_name_or_path,
                                            from_tf=bool('.ckpt' in args.model_name_or_path),
                                            config=config,
                                            cache_dir=args.cache_dir if args.cache_dir else None)    
    else:
        model = model_class(config)
    if args.model_name == 'codebert':
        model = CodeBERT(model, config, tokenizer, args)
        eval_dataset = CodeBertTextDataset(tokenizer, args, args.eval_data_file)
    elif args.model_name == 'graphcodebert':
        model = GraphCodeBERT(model, config, tokenizer, args)
        eval_dataset = GraphCodeBertTextDataset(tokenizer, args, args.eval_data_file)
    elif args.model_name == 'codet5':
        model = CodeT5(model, config, tokenizer, args)
        eval_dataset = CodeT5TextDataset(tokenizer, args, args.eval_data_file)

    checkpoint_prefix = 'checkpoint-best-acc/%s_model.bin' % args.model_name
    output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))
    # print(model)
    if os.path.exists(output_dir):
        try:
            model.load_state_dict(torch.load(output_dir))
        except:
            model.load_state_dict(torch.load(output_dir),strict=False)
    model.to(args.device)

    codebert_mlm = RobertaForMaskedLM.from_pretrained(args.base_model)
    codebert_mlm.to(args.device)
    tokenizer_mlm = RobertaTokenizer.from_pretrained(args.base_model)
    fasttext_model = fasttext.load_model('../code/saved_models/fasttext_model.bin')
    codet5_path = "../code/saved_models/codet5-base"
    t5_tokenizer = RobertaTokenizer.from_pretrained(codet5_path)
    t5_model = T5ForConditionalGeneration.from_pretrained(codet5_path)
    t5_model.to(args.device)

    source_codes = []   
    generated_substitutions = []
    with open(args.eval_data_file) as f:
        for line in f:
            js=json.loads(line.strip())
            # code = ' '.join(js['func'].split())
            code = js['func']
            source_codes.append(code)
            generated_substitutions.append(js['substitutes'])
    assert(len(source_codes) == len(eval_dataset) == len(generated_substitutions))
    print(len(eval_dataset), len(source_codes))

    # attacker = Attacker(args, model, tokenizer, tokenizer_mlm, codebert_mlm, fasttext_model, generated_substitutions)
    # TODO pop_size待确定 种群大小 最大迭代次数
    attacker = PSOAttack(args, model, tokenizer, tokenizer_mlm, codebert_mlm, fasttext_model,t5_model,t5_tokenizer, generated_substitutions, 20, 20)
    # attacker = PSOAttack(args, model, tokenizer, tokenizer_mlm, codebert_mlm, fasttext_model, generated_substitutions)

    # source_codes = []
    # with open(args.eval_data_file) as f:
    #     for line in f:
    #         js = json.loads(line.strip())
    #         code = js['func']
    #         source_codes.append(code)
    #         generated_substitutions.append(js['substitutes'])
    # print(len(eval_dataset), len(source_codes))

    success_attack = 0
    total_cnt = 0
    # for index, example in enumerate(eval_dataset):
    start_time = time.time()
    adv_example=[]
    derta_C = 0
    query_times = 0
    total_codebleu = 0
    diversity = 0
    
    for index, example in enumerate(tqdm(eval_dataset, desc="Processing examples",file=sys.stdout)):
        # if  index < 209:
        #     continue
        code = source_codes[index]
        logger.info(f"测试:{index}\n")
        is_success, final_code, min_gap_prob, query, diver= attacker.attack(
            index,
            example,
            code,
            index
        )
        derta_C += min_gap_prob if min_gap_prob > 0 else 0
        print("Query times in this attack: ", model.query - query_times)
        print("All Query times: ", model.query)
        query_times += model.query
        diversity += diver
        bleu_score = calc_codebleu([code], [final_code], lang="c", weights=(0.25, 0.25, 0.25, 0.25), tokenizer=None)
        total_codebleu += bleu_score['codebleu']
        if is_success >= -1:
            total_cnt += 1
            if is_success >= 1:
                success_attack += 1
            if total_cnt == 0:
                continue
            # print("Success rate: %.2f%%" % ((1.0 * success_attack / total_cnt) * 100))
            # print("Successful items count: ", success_attack)
            # print("Total count: ", total_cnt)
            # print("Index: ", index)
            # print()
            logger.info("Success rate: %.2f%%\n", (1.0 * success_attack / total_cnt) * 100)
            logger.info("Successful items count: %d\n", success_attack)
            logger.info("Total count: %d\n", total_cnt)
            logger.info("Index: %d\n", index)
        else:
            # print("Attack failed!")
            logger.info("Attack failed!\n")

        adv_code_lines = [x + '\\n' for x in final_code.split('\n') if x ]
        adv_code_save = "".join(adv_code_lines)
        adv_code_save = adv_code_save + '\n'
        adv_example.append(adv_code_save)
    
    print(f"Average diversity max: ", round(diversity / success_attack,2))
    print(f"Average diversity min: ", round(diversity / len(eval_dataset),2))
    print(f"ALL examples'average drop: ", round(derta_C / success_attack, 2))
    print(f"ALL examples'average query_times: ", round(query_times / success_attack, 2))
    print("Average Query times baseline: ", round(model.query /  len(eval_dataset),2))
    print("ALL examples time cost: ", round((time.time()-start_time)/60, 2), "min")
    
    # 在最后输出平均CodeBLEU分数
    avg_codebleu = total_codebleu / success_attack if success_attack > 0 else 0
    print(f"Average CodeBLEU score max: {round(avg_codebleu, 4)}")
    print(f"Average CodeBLEU score min: {round(total_codebleu / len(eval_dataset), 4)}")
    
    with open('./codebert_20_20_new_not_all.txt', 'w') as f:
        for example in adv_example:
            f.write(example)
        
if __name__ == '__main__':
    main()
    """
    CUDA_VISIBLE_DEVICES=1 python test.py --eval_data_file=../dataset/test_subs_gan_0_400.jsonl --model_name=codebert --seed=12345 2>&1 | tee codebert_20_20_new_not_all.log

    CUDA_VISIBLE_DEVICES=2 python test.py --eval_data_file=../dataset/test_subs_gan_0_400.jsonl --model_name=codet5 --seed=12345 2>&1 | tee ours_codet5_20_iter_20_notall.log

    CUDA_VISIBLE_DEVICES=1 python test.py --eval_data_file=../dataset/test_subs_gan_0_400.jsonl --model_name=codet5 --seed=12345 2>&1 | tee codet5_20_iter_20_all.log

    CUDA_VISIBLE_DEVICES=2 python test.py --eval_data_file=../dataset/test_subs_gan_0_400.jsonl --model_name=graphcodebert --seed=12345 2>&1 | tee graphcodebert_20_iter_20_all.log
    """