import sys

import logging
import random
import numpy as np
import torch
import pandas as pd
import copy
sys.path.append("../../")
sys.path.append("../../python_parser")
sys.path.append('../code')
import subprocess
import json

from run_parser import get_identifiers,extract_dataflow,remove_comments_and_docstrings,get_example,get_example_batch,remove_strings
import logging
import numpy as np
import pandas as pd
from utils import get_identifier_posistions_from_code,get_masked_code_by_position,CodeDataset, _tokenize,GraphCodeDataset,CodeT5Dataset,isUID
import os
import time
import torch.nn as nn
import copy
import utils
from scipy.spatial.distance import cosine as cosine_distance
from run import CodeBertInputFeatures, GraphCodeBertInputFeatures, CodeT5InputFeatures
from Transform import GenRandomChange
import re
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import Levenshtein

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)

def codebert_convert_code_to_features(code, tokenizer, label, args):
    code = ' '.join(code.split())
    code_tokens = tokenizer.tokenize(code)[:args.block_size-2]
    source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]
    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)
    padding_length = args.block_size - len(source_ids)
    source_ids += [tokenizer.pad_token_id] * padding_length

    return CodeBertInputFeatures(source_tokens, source_ids, 0, label)


def graphcodebert_convert_code_to_features(code, tokenizer, label, args):
    code = ' '.join(code.split())
    dfg, index_table, code_tokens = extract_dataflow(code, "c")
    code_tokens = [tokenizer.tokenize('@ ' + x)[1:] if idx != 0 else tokenizer.tokenize(x) for idx, x in enumerate(code_tokens)]
    ori2cur_pos = {}
    ori2cur_pos[-1] = (0, 0)
    for i in range(len(code_tokens)):
        ori2cur_pos[i] = (ori2cur_pos[i - 1][1], ori2cur_pos[i - 1][1] + len(code_tokens[i]))
    code_tokens = [y for x in code_tokens for y in x]

    code_tokens = code_tokens[:args.code_length + args.data_flow_length - 2 - min(len(dfg), args.data_flow_length)]
    source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]
    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)
    position_idx = [i + tokenizer.pad_token_id + 1 for i in range(len(source_tokens))]
    dfg = dfg[:args.code_length + args.data_flow_length - len(source_tokens)]
    source_tokens += [x[0] for x in dfg]
    position_idx += [0 for x in dfg]
    source_ids += [tokenizer.unk_token_id for x in dfg]
    padding_length = args.code_length + args.data_flow_length - len(source_ids)
    position_idx += [tokenizer.pad_token_id] * padding_length
    source_ids += [tokenizer.pad_token_id] * padding_length

    reverse_index = {}
    for idx, x in enumerate(dfg):
        reverse_index[x[1]] = idx
    for idx, x in enumerate(dfg):
        dfg[idx] = x[:-1] + ([reverse_index[i] for i in x[-1] if i in reverse_index],)
    dfg_to_dfg = [x[-1] for x in dfg]
    dfg_to_code = [ori2cur_pos[x[1]] for x in dfg]
    length = len([tokenizer.cls_token])
    dfg_to_code = [(x[0] + length, x[1] + length) for x in dfg_to_code]

    return GraphCodeBertInputFeatures(source_tokens, source_ids, position_idx, dfg_to_code, dfg_to_dfg, 0, label)


def codet5_convert_code_to_features(code, tokenizer, label, args):
    code = ' '.join(code.split())
    code_tokens = tokenizer.tokenize(code)[:args.block_size-2]
    source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]
    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)
    padding_length = args.block_size - len(source_ids)
    source_ids += [tokenizer.pad_token_id] * padding_length

    return CodeT5InputFeatures(source_tokens, source_ids, 0, label)


def get_embeddings(code, variables, tokenizer_mlm, codebert_mlm):
    new_code = copy.deepcopy(code)
    chromesome = {}
    for i in variables:
        chromesome[i] = '<unk>'
    new_code = get_example_batch(new_code, chromesome, "c")
    _, _, code_tokens = get_identifiers(remove_comments_and_docstrings(new_code, "c"), "c")
    processed_code = " ".join(code_tokens)
    words, sub_words, keys = _tokenize(processed_code, tokenizer_mlm)
    sub_words = [tokenizer_mlm.cls_token] + sub_words[:512 - 2] + [tokenizer_mlm.sep_token]
    input_ids_ = torch.tensor([tokenizer_mlm.convert_tokens_to_ids(sub_words)])
    with torch.no_grad():
        embeddings = codebert_mlm.roberta(input_ids_.to('cuda'))[0]

    return embeddings

NONREACHABLECODE = ['if(0){char *input;char *buffer = (char*)malloc(64 * sizeof(char));strcpy(buffer, input); }', 'int x=0; if(!(x*(x-1) % 2 == 0)) { x = (x+3)/x  }', 'int x=0,y; bool flag_is_true = false; if(flag_is_true) { char *dest = (char*)malloc(64 * sizeof(char)); char *user_input =  strcat(dest, user_input); }', 'int x=0,y; if(!(x*(x-1) % 2 == 0)) { float n=0.0; if(n>10) char *dest = (char*)malloc(64 * sizeof(char)); char *user_input = "input"; strcat(dest, user_input); else n=0; }', 'int x=0,y; if(!(x*(x-1) % 2 == 0)) { int temp=0; int *asdfwq; while(temp<10) { temp=temp+1; if(temp==9) asdfwq[temp] = temp; break;}}', 'int x=0,y; if(!(x*(x+1) % 2 == 0)){ double temp=0.0; if(temp==3) char *str; { char *temp = "Hello, World!"; tr = temp; }  return 0; }']

def loss(embedding_a, embedding_b):
        '''
        compute the squared distance between embedding_a and embedding_b
        '''        
        return nn.MSELoss()(embedding_a.to('cuda'), embedding_b.to('cuda')).item()

def get_insert_masked_code(code, pos):
    """在指定位置插入[UNK]序列"""
    splited_code = code.split(';')
    # 插入一串[UNK]标记作为死代码的占位符
    unk_sequence = ' '.join(['<unk>' for _ in range(5)])  # 使用5个UNK标记
    splited_code.insert(pos, unk_sequence)
    return ';'.join(splited_code)

def get_inserted_code(code, pos, print_statement):
            splited_code = code.split(';')
            splited_code.insert(pos, '{}'.format(print_statement))
            inserted_code_str = ''
            for line in splited_code:
                inserted_code_str += (line + ';')
            return inserted_code_str

def get_input_ids(processed_masked_code,tokenizer_mlm, codebert_mlm,args):
    code_tokens=tokenizer_mlm.tokenize(processed_masked_code)[:args.block_size-2]
    source_tokens =[tokenizer_mlm.cls_token]+code_tokens+[tokenizer_mlm.sep_token]
    source_ids =  tokenizer_mlm.convert_tokens_to_ids(source_tokens)
    padding_length = args.block_size - len(source_ids)
    source_ids+=[tokenizer_mlm.pad_token_id]*padding_length
    return source_ids

def find_vulnerable_positions(ori_code, tokenizer_mlm, codebert_mlm, args, top_k=None):
    """
    Find vulnerable positions for code insertion using cosine similarity of [CLS] representations
    
    Args:
        ori_code: 原始代码
        tokenizer_mlm: CodeBERT tokenizer
        codebert_mlm: 预训练的CodeBERT模型(未微调)
        args: 参数配置
        top_k: 返回前k个最脆弱的位置，如果为None则返回所有位置
    
    Returns:
        sorted_positions: 按脆弱性排序的位置列表（分数越低表示位置越脆弱）
    """
    # 获取所有可能的插入位置（分号后的位置）
    insert_positions = len(re.findall(';', ori_code))
    position_scores = []
    
    # 获取原始代码的[CLS]表示
    ori_feature = get_input_ids(ori_code, tokenizer_mlm, codebert_mlm, args)
    with torch.no_grad():
        # 获取[CLS]表示 (取第一个token的表示)
        rc = codebert_mlm(torch.tensor(ori_feature).unsqueeze(0).to(args.device))[0][:,0,:].detach().cpu()
        
    # 对每个位置计算分数
    for pos in range(insert_positions):
        # 在位置pos插入[UNK]序列
        modified_code = get_insert_masked_code(ori_code, pos)
        modified_feature = get_input_ids(modified_code, tokenizer_mlm, codebert_mlm, args)
        
        with torch.no_grad():
            # 获取修改后代码的[CLS]表示
            rc_prime = codebert_mlm(torch.tensor(modified_feature).unsqueeze(0).to(args.device))[0][:,0,:].detach().cpu()
            
            # 计算余弦相似度作为位置得分
            score = torch.nn.functional.cosine_similarity(rc, rc_prime, dim=1)[0].item()
            position_scores.append((pos, score))
    
    # 按分数从低到高排序（分数越低表示位置越脆弱）
    sorted_positions = sorted(position_scores, key=lambda x: x[1])
    
    # 返回前k个位置，如果k为None则返回所有位置
    if top_k is not None:
        sorted_positions = sorted_positions[:top_k]
    
    # 只返回位置索引
    return [pos for pos, _ in sorted_positions]
    
def find_dissimilar_codes(target_code, code_pool, tokenizer_mlm, codebert_mlm, args, K=30, top_k=30):
    """
    从代码池中找出与目标代码最不相似的代码
    
    Args:
        target_code: 目标代码（要被修改的代码）
        code_pool: 候选代码池（不在训练/测试集中的代码）
        tokenizer_mlm: CodeBERT tokenizer
        codebert_mlm: 预训练的CodeBERT模型
        args: 参数配置
        K: 随机选择的代码数量
        top_k: 返回最不相似的代码数量
    
    Returns:
        sorted_codes: 按不相似度排序的代码列表（分数越低表示越不相似）
    """
    # 随机选择K个代码
    if len(code_pool) > K:
        selected_codes = random.sample(code_pool, K)
    else:
        selected_codes = code_pool
    
    # 获取目标代码的[CLS]表示
    target_feature = get_input_ids(target_code, tokenizer_mlm, codebert_mlm, args)
    with torch.no_grad():
        rc = codebert_mlm(torch.tensor(target_feature).unsqueeze(0).to(args.device))[0][:,0,:].detach().cpu()
        
    dissimilarity_scores = []
    
    # 计算每个候选代码与目标代码的不相似度
    for code in selected_codes:
        # 获取候选代码的[CLS]表示
        code_feature = get_input_ids(code, tokenizer_mlm, codebert_mlm, args)
        with torch.no_grad():
            rci = codebert_mlm(torch.tensor(code_feature).unsqueeze(0).to(args.device))[0][:,0,:].detach().cpu()
            
            # 计算余弦相似度
            similarity = torch.nn.functional.cosine_similarity(rc, rci, dim=1)[0].item()
            # 转换为不相似度（1 - 相似度）
            dissimilarity = 1 - similarity
            dissimilarity_scores.append((code, dissimilarity))
    
    # 按不相似度从高到低排序
    sorted_codes = sorted(dissimilarity_scores, key=lambda x: x[1], reverse=True)
    
    # 返回前top_k个最不相似的代码
    return sorted_codes[:top_k]

def get_dissimilar_code_pool():
    code_pool = []
    with open('../dataset/test_subs_400_800.jsonl', 'r') as f:
        for line in f:
            data = json.loads(line)
            code_pool.append(data['func'])
    return code_pool

def extract_high_attention_statements(code, tokenizer_mlm, codebert_mlm, args):
    """
    从代码中提取具有最高注意力分数的语句
    
    Args:
        code: 源代码
        tokenizer_mlm: CodeBERT tokenizer
        codebert_mlm: 预训练的CodeBERT模型
        args: 参数配置
    
    Returns:
        best_statement: 具有最高注意力分数的语句
        max_attention_score: 最高注意力分数
    """
    # 分割代码为语句
    statements = code.split(';')
    if len(statements) <= 1:
        return None, 0, -1
    
    best_statement = None
    max_attention_score = 0
    best_idx = None
    max_length = 512  # CodeBERT的最大序列长度
    # 对每个语句计算注意力分数
    for idx, statement in enumerate(statements):
        if not statement.strip():
            continue
            
        # 对语句进行分词
        tokens = tokenizer_mlm.tokenize(statement)
        if not tokens:
            continue
            

        # 截断过长的序列
        tokens = tokens[:max_length-2]  # 留出[CLS]和[SEP]的位置
        # 添加特殊标记
        tokens = [tokenizer_mlm.cls_token] + tokens + [tokenizer_mlm.sep_token]
        token_ids = tokenizer_mlm.convert_tokens_to_ids(tokens)
        
        # 填充到固定长度
        padding_length = max_length - len(token_ids)
        token_ids += [tokenizer_mlm.pad_token_id] * padding_length
        # 准备输入
        input_ids = torch.tensor([token_ids]).to(args.device)
        attention_mask = torch.ones(1, len(token_ids)).to(args.device)
        attention_mask[0, len(tokens):] = 0  # 对填充部分的attention mask设为0
        
        try:
            # 获取注意力分数
            with torch.no_grad():
                outputs = codebert_mlm(
                    input_ids,
                    attention_mask=attention_mask,
                    output_attentions=True
                )
                # 获取倒数第二层的注意力权重
                attention_weights = outputs.attentions[-2]
                
                # 计算每个token的平均注意力分数
                # 只考虑非填充部分的注意力分数
                avg_attention = attention_weights[0, :, 0, :len(tokens)].mean(dim=0)
                
                # 获取最高注意力分数
                statement_max_score = avg_attention.max().item()
                
                if statement_max_score > max_attention_score:
                    max_attention_score = statement_max_score
                    best_statement = statement.strip()
                    best_idx = idx
        except Exception as e:
            print(f"Error processing statement {idx}: {e}")
            continue
    
    return best_statement, max_attention_score, best_idx

def generate_dead_code(target_code, dissimilar_codes, tokenizer_mlm, codebert_mlm, args):
    """
    生成死代码列表
    
    Args:
        target_code: 目标代码
        dissimilar_codes: 不相似代码列表
        tokenizer_mlm: CodeBERT tokenizer
        codebert_mlm: 预训练的CodeBERT模型
        args: 参数配置
    
    Returns:
        dead_code_list: 死代码列表
    """
    dead_code_list = []
    
    for code, _ in dissimilar_codes:
        # 提取具有最高注意力分数的语句
        best_statement, attention_score, _ = extract_high_attention_statements(
            code, 
            tokenizer_mlm, 
            codebert_mlm, 
            args
        )
        
        if best_statement:
            # 创建死代码（将语句包装为变量声明）
            dead_code = f"string var = \"{best_statement}\""
            dead_code_list.append((dead_code, attention_score))
    
    # 按注意力分数排序
    dead_code_list.sort(key=lambda x: x[1], reverse=True)
    
    return [code for code, _ in dead_code_list]




class DIPAttack(object):
    def __init__(self, args, model_tgt, tokenizer_tgt, tokenizer_mlm, codebert_mlm, fasttext_model,t5_model,t5_tokenizer, generated_substitutions, pop_size,max_iters) -> None:
        self.args = args
        self.model_tgt = model_tgt
        self.tokenizer_tgt = tokenizer_tgt
        self.tokenizer_mlm = tokenizer_mlm
        self.codebert_mlm = codebert_mlm
        self.fasttext_model = fasttext_model
        self.t5_model = t5_model
        self.t5_tokenizer = t5_tokenizer
        self.substitutions = generated_substitutions
        self.pop_size = pop_size
        self.max_iters = max_iters
        self.particle_positions = []  # 用于记录每次迭代中粒子的位置
        self.diversity_indices = []  # 用于记录每次迭代的多样性指数

    def calculate_diversity(self, diversity_indices):
        # 计算种群中每对粒子之间的距离
        distances = []
        for i in range(len(diversity_indices)):
            for j in range(i + 1, len(diversity_indices)):
                # Calculate Levenshtein distance between two particles
                distance = Levenshtein.distance(' '.join(diversity_indices[i]), ' '.join(diversity_indices[j]))
                distances.append(distance)
        # 返回平均距离作为多样性指数
        return np.mean(distances) if len(distances) > 0 else 0
   
    def predict_batch(self,code_batch, labels):
        replace_examples = []
        for idx, code in enumerate(code_batch):
            replace_examples.append(self.get_feature(code, labels[idx]))

        # 评估新生成的对抗样本
        if self.args.model_name == 'codebert':
            new_dataset = CodeDataset(replace_examples)
        elif self.args.model_name == 'graphcodebert':
            new_dataset = GraphCodeDataset(replace_examples, self.args)
        elif self.args.model_name == 'codet5':
            new_dataset = CodeT5Dataset(replace_examples)
        logits, preds = self.model_tgt.get_results(new_dataset, self.args.eval_batch_size)
       
        return logits, preds

    
    def get_feature(self,temp_code,label):
        replace_examples = []
        if self.args.model_name == 'codebert':
            new_feature = codebert_convert_code_to_features(temp_code, self.tokenizer_tgt,label, self.args)
        elif self.args.model_name == 'graphcodebert':
            new_feature = graphcodebert_convert_code_to_features(temp_code, self.tokenizer_tgt,label, self.args)
        elif self.args.model_name == 'codet5':
            new_feature = codet5_convert_code_to_features(temp_code, self.tokenizer_tgt,label, self.args)
        
        return new_feature


    def find_most_used_var(self,code):
        """找出代码中最常用的变量名"""
        # 使用正则表达式匹配变量名
        var_pattern = r'\b[a-zA-Z_]\w*\b'
        vars = re.findall(var_pattern, code)
        
        # 统计变量出现次数
        var_count = {}
        for var in vars:
            if var not in ['String', 'int', 'float', 'double', 'char', 'void', 'if', 'for', 'while']:
                var_count[var] = var_count.get(var, 0) + 1
        
        # 返回出现次数最多的变量名
        if var_count:
            most_used_var = max(var_count.items(), key=lambda x: x[1])[0]
            return most_used_var
        return "var"  # 默认变量名

    def wrap_snippet_as_dead_code(self,snippet, base_var_name):
        """将代码片段包装为死代码"""
        # 生成未使用的变量名
        var_name = f"{base_var_name}_2"
        
        # 处理snippet中的特殊字符
        escaped_snippet = snippet.replace('"', '\\"').replace('\n', ' ')
        
        # 包装为字符串变量声明
        dead_code = f'String {var_name} = "{escaped_snippet}";'
        return dead_code

    def generate_and_insert_dead_code(self,orig_code, tokenizer_mlm, codebert_mlm, label,args, current_prob):
        """生成并插入死代码直到攻击成功"""

        most_gap_prob = 0
        # 1. 获取代码池中的不相似代码
        dissimilar_codes = find_dissimilar_codes(
            orig_code,
            get_dissimilar_code_pool(),
            tokenizer_mlm,
            codebert_mlm,
            args,
            K=30
        )
        
        # 2. 找出最常用的变量名
        base_var_name = self.find_most_used_var(orig_code)
        
        # 3. 找出脆弱位置
        vulnerable_positions = find_vulnerable_positions(
            orig_code,
            tokenizer_mlm,
            codebert_mlm,
            args
        )
        
        # 4. 从每个不相似代码中提取高注意力语句并生成死代码
        dead_code_list = []
        for code, _ in dissimilar_codes:
            best_statement, _, _ = extract_high_attention_statements(
                code,
                tokenizer_mlm,
                codebert_mlm,
                args
            )
            if best_statement:
                dead_code = self.wrap_snippet_as_dead_code(best_statement, base_var_name)
                dead_code_list.append(dead_code)
        
        # 5. 尝试在每个脆弱位置插入死代码
        def insert_code_at_position(code, insert_code, position):
            """在指定位置插入代码"""
            lines = code.split(';')
            lines.insert(position, insert_code)
            return ';'.join(lines)
        
        # 记录最佳结果
        best_adv_code = None
        best_success = False
        
        # 对每个死代码和每个位置尝试插入
        for dead_code in dead_code_list:
            for pos in vulnerable_positions:
                # 生成新的对抗样本
                adv_code = insert_code_at_position(orig_code, dead_code, pos)
                
                # 评估攻击效果
                logits, preds = self.predict_batch([adv_code], [label])
                most_gap_prob = max(most_gap_prob, current_prob - logits[0][label])
                if preds[0] != label:  # 攻击成功
                    return True, adv_code, dead_code, pos, most_gap_prob
                
                # 更新最佳结果（如果需要跟踪最佳尝试）
                if best_adv_code is None or logits[0][label] < best_logits:
                    self.diversity_indices.append(adv_code)
                    best_adv_code = adv_code
                    best_logits = logits[0][label]
        
        return False, best_adv_code, None, None, most_gap_prob

    def dead_code_attack(self, orig_code, label, tokenizer_mlm, codebert_mlm, args, current_prob):
        """主攻击函数"""
        print("Starting dead code attack...")
        
        # 执行攻击
        success, adv_code, used_dead_code, insert_position, most_gap_prob = self.generate_and_insert_dead_code(
            orig_code,
            tokenizer_mlm,
            codebert_mlm,
            label,
            args,
            current_prob
        )
        
        if success:
            print("Attack succeeded!")
            print(f"Inserted dead code: {used_dead_code}")
            print(f"At position: {insert_position}")
        else:
            print("Attack failed. Returning best attempt.")
        
        return success, adv_code, most_gap_prob

    def attack(self, idx, example, orig_code, pos_tags=None):
        self.particle_positions = []  # 用于记录每次迭代中粒子的位置
        self.diversity_indices = []  # 用于记录每次迭代的多样性指数
        # 先预测
        logits, preds = self.model_tgt.get_results([example], self.args.eval_batch_size)
        orig_prob = logits[0]
        orig_label = preds[0]
        current_prob = max(orig_prob)
        if self.args.model_name == 'codebert':
            true_label = example[1].item()
        elif self.args.model_name == 'graphcodebert':
            true_label = example[3].item()
        elif self.args.model_name == 'codet5':
            true_label = example[1].item()
        query_times = 1
        flag_success = False  # whether we have found the first adversarial example in our algorithm
        is_success = 0

        # 对应代码的变量名替换
        substituions = self.substitutions[idx]
        identifiers,code_tokens = get_identifiers(orig_code, "c")
        variable_names = list(substituions.keys())
        processed_code = " ".join(code_tokens)
        # 分别是代码中的分词，分词的token，位置
        words, sub_words, keys = _tokenize(processed_code, self.tokenizer_mlm)

        if orig_label != true_label:  # the original code is misclassified
            print("The original code is misclassified.")
            is_success = -4
            return is_success, orig_code, 0, query_times, 0
        elif len(variable_names) == 0:  # no identifier in the code
            print("No variable_names in the code.")
            is_success = -3
            return is_success, orig_code, 0, query_times, 0
        
        # min_prob = current_prob
        names_positions_dict = get_identifier_posistions_from_code(
            words, variable_names
        )
        if len(names_positions_dict) == 0:
            print("No identifiers in the code.")
            is_success = -3
            return is_success, orig_code, 0, query_times, 0

        '''DIP 具体逻辑实现'''
        # TODO Vulnerable position selection
        # 获取脆弱位置
        # sorted_id = find_vulnerable_positions(orig_code, self.tokenizer_mlm, self.codebert_mlm, self.args)

        # TODO Dissimilar code selection
        # 获取不相似的代码池
        # code_pool = get_dissimilar_code_pool()
        # # 获取不相似的代码
        # dissimilar_codes = find_dissimilar_codes(
        #     orig_code,
        #     code_pool,
        #     self.tokenizer_mlm,
        #     self.codebert_mlm,
        #     self.args
        # )

        # TODO Snippet extraction
        # 生成死代码列表
        # dead_code_list = generate_dead_code(
        #     orig_code,
        #     dissimilar_codes,
        #     self.tokenizer_mlm,
        #     self.codebert_mlm,
        #     self.args
        # )

        # TODO Adversarial code generation
        # 这一个函数包含了之前的所有步骤
        most_gap_prob = 0
        success, adv_code, most_gap_prob = self.dead_code_attack(
            orig_code,
            true_label,
            self.tokenizer_mlm,
            self.codebert_mlm,
            self.args,
            current_prob
        )
        if success:
            is_success = 1
        else:
            is_success = 0

        return is_success, adv_code, most_gap_prob, query_times, self.calculate_diversity(self.diversity_indices)
    
if __name__ == '__main__':
    pass