import sys

import logging
import random
import numpy as np
import torch
import pandas as pd
import copy
sys.path.append("../")
sys.path.append("../python_parser")

from run_parser import get_identifiers,extract_dataflow,remove_comments_and_docstrings,get_example,get_example_batch
import logging
import numpy as np
import pandas as pd
from utils import get_identifier_posistions_from_code,get_masked_code_by_position,CodeDataset, _tokenize,get_example_batch,GraphCodeDataset,CodeT5Dataset
import os
import time
import torch.nn as nn
import copy
import utils
from scipy.spatial.distance import cosine as cosine_distance
from run import CodeBertInputFeatures, GraphCodeBertInputFeatures, CodeT5InputFeatures

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)

def codebert_convert_code_to_features(code, tokenizer, label, args):
    code = ' '.join(code.split())
    code_tokens = tokenizer.tokenize(code)[:args.block_size-2]
    source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]
    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)
    padding_length = args.block_size - len(source_ids)
    source_ids += [tokenizer.pad_token_id] * padding_length

    return CodeBertInputFeatures(source_tokens, source_ids, 0, label)


def graphcodebert_convert_code_to_features(code, tokenizer, label, args):
    code = ' '.join(code.split())
    dfg, index_table, code_tokens = extract_dataflow(code, "c")
    code_tokens = [tokenizer.tokenize('@ ' + x)[1:] if idx != 0 else tokenizer.tokenize(x) for idx, x in
                   enumerate(code_tokens)]
    ori2cur_pos = {}
    ori2cur_pos[-1] = (0, 0)
    for i in range(len(code_tokens)):
        ori2cur_pos[i] = (ori2cur_pos[i - 1][1], ori2cur_pos[i - 1][1] + len(code_tokens[i]))
    code_tokens = [y for x in code_tokens for y in x]

    code_tokens = code_tokens[:args.code_length + args.data_flow_length - 2 - min(len(dfg), args.data_flow_length)]
    source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]
    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)
    position_idx = [i + tokenizer.pad_token_id + 1 for i in range(len(source_tokens))]
    dfg = dfg[:args.code_length + args.data_flow_length - len(source_tokens)]
    source_tokens += [x[0] for x in dfg]
    position_idx += [0 for x in dfg]
    source_ids += [tokenizer.unk_token_id for x in dfg]
    padding_length = args.code_length + args.data_flow_length - len(source_ids)
    position_idx += [tokenizer.pad_token_id] * padding_length
    source_ids += [tokenizer.pad_token_id] * padding_length

    reverse_index = {}
    for idx, x in enumerate(dfg):
        reverse_index[x[1]] = idx
    for idx, x in enumerate(dfg):
        dfg[idx] = x[:-1] + ([reverse_index[i] for i in x[-1] if i in reverse_index],)
    dfg_to_dfg = [x[-1] for x in dfg]
    dfg_to_code = [ori2cur_pos[x[1]] for x in dfg]
    length = len([tokenizer.cls_token])
    dfg_to_code = [(x[0] + length, x[1] + length) for x in dfg_to_code]

    return GraphCodeBertInputFeatures(source_tokens, source_ids, position_idx, dfg_to_code, dfg_to_dfg, 0, label)


def codet5_convert_code_to_features(code, tokenizer, label, args):
    code = ' '.join(code.split())
    code_tokens = tokenizer.tokenize(code)[:args.block_size-2]
    source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]
    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)
    padding_length = args.block_size - len(source_ids)
    source_ids += [tokenizer.pad_token_id] * padding_length

    return CodeT5InputFeatures(source_tokens, source_ids, 0, label)


def get_embeddings(code, variables, tokenizer_mlm, codebert_mlm):
    new_code = copy.deepcopy(code)
    chromesome = {}
    for i in variables:
        chromesome[i] = '<unk>'
    new_code = get_example_batch(new_code, chromesome, "c")
    _, _, code_tokens = get_identifiers(remove_comments_and_docstrings(new_code, "c"), "c")
    processed_code = " ".join(code_tokens)
    words, sub_words, keys = _tokenize(processed_code, tokenizer_mlm)
    sub_words = [tokenizer_mlm.cls_token] + sub_words[:512 - 2] + [tokenizer_mlm.sep_token]
    input_ids_ = torch.tensor([tokenizer_mlm.convert_tokens_to_ids(sub_words)])
    with torch.no_grad():
        embeddings = codebert_mlm.roberta(input_ids_.to('cuda'))[0]

    return embeddings

# def convert_code_to_features(code, tokenizer, label, args):
#     code=' '.join(code.split())
#     code_tokens=tokenizer.tokenize(code)[:args.block_size-2]
#     source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]
#     source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)
#     padding_length = args.block_size - len(source_ids)
#     source_ids+=[tokenizer.pad_token_id]*padding_length
#     return InputFeatures(source_tokens,source_ids, 0, label)

# def get_importance_score(self, code, words_list: list, sub_words: list, variable_names: list, model_type='classification', target_example=None):
#         '''
#         Compute the importance score of each variable. Replace each variable to see its influence on the output confidence
#         '''
#         # label: example[1] tensor(1)
#         # 1. 过滤掉所有的keywords.
#         positions = get_identifier_posistions_from_code(words_list, variable_names)
#         # 需要注意大小写.
#         if len(positions) == 0:
#             ## 没有提取出可以mutate的position
#             print('Error! (in get_importance_score) len(positions)==0')
#             return None, None, None

#         new_examples = []

#         # 2. 得到Masked_tokens, whose length equals to the number of the manipulated code
#         masked_token_list, replace_token_positions = get_masked_code_by_position(words_list, positions)
#         # replace_token_positions 表示着，哪一个位置的token被替换了.

#         for index, tokens in enumerate([words_list] + masked_token_list):
#             # words_list denotes the original code?
#             new_code = ' '.join(tokens)
#             new_feature = convert_code_to_features(new_code)
#             new_examples.append(new_feature.input_ids)
#         # 3. 将他们转化成features
#         embeddings_list = []
#         for example in new_examples:
#             # detach here is necessary to aviod OOM
#             embeddings_list.append(self.model_sub(torch.tensor(example).unsqueeze(0).to(self.args.device))[0].detach().cpu())   
#         embeddings = torch.stack(embeddings_list)


#         if self.targeted:
#             target_embeddings = self.model_sub(torch.tensor(target_example[0]).unsqueeze(0).to(self.args.device))[0]

#         orig_embeddings = embeddings[0]
#         importance_score = []
#         for embed in embeddings[1:]:
#             if self.targeted:
#                 importance_score.append(self.loss(orig_embeddings, target_embeddings) - self.loss(embed, target_embeddings))
#             else:
#                 importance_score.append(self.loss(embed, orig_embeddings))

#         return importance_score, replace_token_positions, positions



class PSOAttack(object):
    def __init__(self, args, model_tgt, tokenizer_tgt, tokenizer_mlm, codebert_mlm, fasttext_model, generated_substitutions, pop_size,max_iters) -> None:
        self.args = args
        self.model_tgt = model_tgt
        self.tokenizer_tgt = tokenizer_tgt
        self.tokenizer_mlm = tokenizer_mlm
        self.codebert_mlm = codebert_mlm
        self.fasttext_model = fasttext_model
        self.substitutions = generated_substitutions
        self.pop_size = pop_size
        self.max_iters = max_iters

    
    def do_replace(self, cur_code, tgt_word, subsitute_word):
        """
        将代码 cur_code 的 cur_code 替换为新词 subsitute_word,返回修改后的代码
        """
        new_code = copy.deepcopy(cur_code)
        new_code = get_example(" ".join(new_code), tgt_word,subsitute_word, "c")
        return new_code
    
    def predict_batch(self,code_batch, labels):
        replace_examples = []
        for idx, code in enumerate(code_batch):
            replace_examples.append(self.get_feature(code, labels[idx]))

        # 评估新生成的对抗样本
        if self.args.model_name == 'codebert':
            new_dataset = CodeDataset(replace_examples)
        elif self.args.model_name == 'graphcodebert':
            new_dataset = GraphCodeDataset(replace_examples, self.args)
        elif self.args.model_name == 'codet5':
            new_dataset = CodeT5Dataset(replace_examples)
        logits, preds = self.model_tgt.get_results(new_dataset, self.args.eval_batch_size)
       
        return logits, preds

    
    def get_feature(self,temp_code,label):
        replace_examples = []
        if self.args.model_name == 'codebert':
            new_feature = codebert_convert_code_to_features(temp_code, self.tokenizer_tgt,label, self.args)
        elif self.args.model_name == 'graphcodebert':
            new_feature = graphcodebert_convert_code_to_features(temp_code, self.tokenizer_tgt,label, self.args)
        elif self.args.model_name == 'codet5':
            new_feature = codet5_convert_code_to_features(temp_code, self.tokenizer_tgt,label, self.args)
        
        return new_feature

       
        
        # tem=self.model.predict(np.array([sentence]))[0]
        # self.invoke_dict[tuple(sentence)]=tem
        # return tem
    def select_best_replacement(self, pos, cur_code, orig_code, label, replace_list,current_prob):
        """ Select the most effective replacement to word at pos (pos)
        in (cur_code) between the words in replace_list 
        对于给定位置 pos,在候选替换词 replace_list 中找到最能改变目标模型预测的词
        过比较修改后的预测与原始预测，选择最有效的替换词
        """
        # Keep only top_n
        # replace_list = replace_list[:self.top_n]
        # new_x_list = new_x_list[:self.top_n]
        # new_x_preds = new_x_preds[:self.top_n,:]

        new_code_list = [self.do_replace(orig_code,cur_code, var_name) for var_name in replace_list]
        labels = [label for _ in range(len(new_code_list))]
        # [[logits, preds], [logits, preds], ...]
        logits, preds = self.predict_batch(new_code_list,labels)


        for index, temp_prob in enumerate(logits):
            temp_label = preds[index]
            if temp_label != labels[index]:
                print("rename in select_best_replacement %s SUCCESS! (%.5f => %.5f)" % ('>>', current_prob, temp_prob[labels[index]]), flush=True)
                return 1, new_code_list[index], current_prob - temp_prob[label]
            else:
                if min_prob > temp_prob[label]:
                    min_prob = temp_prob[label]
                    final_code = replace_list[index]
        return -1, final_code, current_prob - min_prob

        # code_scores = new_code_preds[:, label]
        # logits, preds = self.predict(cur_code)[label]
        # orig_score = 0
        # new_x_scores = code_scores - orig_score
        # Eliminate not that clsoe words

        # if (np.max(new_x_scores) > 0):
        #     best_id = np.argsort(new_x_scores)[-1]
        #     if np.argmax(new_code_preds[best_id]) == label:
        #         return [1, new_code_list[best_id]]
        #     return [code_scores[best_id], new_code_list[best_id]]
        # return [orig_score, cur_code]


    def perturb(self, cur_code, orig_code, substituions, sorted_list_of_names, label,current_prob):
        """
        选择一个词位置，并根据选择的概率 sorted_list_of_names 选取一个候选词，将原始词替换为该候选词
        返回修改后的代码及其预测结果
        """
        scores = []
        print(f"sorted_list_of_names:{sorted_list_of_names}")
        for name, replacements in sorted_list_of_names.items():
            sub_scores = []
            for sub_name, score in replacements:
                sub_scores.append(score)
            sub_scores = np.array(sub_scores,dtype=np.float64)
            scores.append([name,sub_scores.sum()])
        # 确保得分归一化为概率分布
        scores = np.array(scores)
        scores = np.array([score[1] for score in scores], dtype=np.float64)
        probabilities = scores / scores.sum()
        rand_idx = np.random.choice(len(scores), 1, p=probabilities)[0]
        # chosen_name, chosen_score = None, None
        replace_list=[]
        cumulative_idx = 0
        for name, replacements in sorted_list_of_names.items():
            if cumulative_idx == rand_idx:
                for sub_name, score in replacements:
                    replace_list.append(sub_name)
            cumulative_idx += 1
        print(f"rand_idx:{rand_idx}\n replace_list:{replace_list}")

        # while cur_code[rand_idx] != orig_code[rand_idx] and np.sum(orig_code != cur_code) < np.sum(np.sign(sorted_list_of_names)):
        #     rand_idx = np.random.choice(len(scores), 1, p=probabilities)[0]
        return self.select_best_replacement(rand_idx, cur_code, orig_code, label, replace_list,current_prob)
    
        # identifiers_len = sorted_list_of_names.shape[0]
        # rand_idx = np.random.choice(identifiers_len, 1, p=sorted_list_of_names)[0]
        # while cur_code[rand_idx] != orig_code[rand_idx] and np.sum(orig_code != cur_code) < np.sum(np.sign(sorted_list_of_names)):
        #     rand_idx = np.random.choice(identifiers_len, 1, p=sorted_list_of_names)[0]

        # replace_list = substituions[rand_idx]
        # return self.select_best_replacement(rand_idx, cur_code, orig_code, label, replace_list)
        

    def perturb_init(self, cur_code, orig_code, substituions, sorted_list_of_names, label, idx,current_prob,variable_names):
        """
        选择一个词位置，并根据选择的概率 sorted_list_of_names 选取一个候选词，将原始词替换为该候选词
        返回修改后的代码及其预测结果
        """

        # 按照idx选择
        # print(f"sorted_list_of_names:{sorted_list_of_names}")
        # TODO 替换测太少了？？？
        tgt_word = variable_names[idx % len(variable_names)]
        replace_word = sorted_list_of_names[tgt_word][0][0]
        print(f"replace_word:{replace_word}") 
        adv_code=self.do_replace(cur_code, tgt_word, replace_word)
        logits, preds=self.predict_batch([adv_code], [label])
        print(f"logits:{logits}, preds:{preds}")
        # 获得预测的标签
        temp_label = preds[0]
        temp_prob = logits[0]
        if temp_label != label:
            print("rename in init %s SUCCESS! (%.5f => %.5f)" % ('>>', current_prob, temp_prob[label]), flush=True)
            is_success = 1
        else:
            is_success = -1
        
        # replace_list:['virtio_blk_id', 0.38641333318782123], ['prefix_code', 0.38034662417951737]]
        # is_success, adv_code, change_prob=self.select_best_replacement(idx, cur_code, orig_code, label, replace_word,current_prob)
        
        return is_success, adv_code, current_prob - temp_prob[label]

    def generate_population(self, orig_code, substituions, sorted_list_of_names, label, pop_size,current_prob,variable_names):
        """
        每个种群个体都是通过对原始文本 orig_code 进行扰动得到的，扰动通过 perturb 方法实现
        返回扰动后的代码及其预测结果
        """
        pop = []
        pop_scores=[]
        adv_code = copy.deepcopy(orig_code)
        for idx in range(pop_size):
            is_success, adv_code, min_gap_prob = self.perturb_init(adv_code, orig_code, substituions, sorted_list_of_names, label,idx,current_prob,variable_names)
            if is_success == 1:
                return is_success, adv_code, min_gap_prob
            else:
                pop_scores.append(min_gap_prob)
                pop.append(adv_code)
        return is_success,pop,pop_scores

    def turn(self, code1, code2, prob, identifiers_len):
        """
        根据给定的概率 prob,将文本 x1 的一些词替换到 x2 中，生成新的文本
        """
        # TODO 只能修改变量
        x_new = copy.deepcopy(code2)
        for i in range(identifiers_len):
            if np.random.uniform() < prob[i]:
                x_new[i] = code1[i]
        return x_new


    def norm(self, n):
        """
        对给定的列表 n 进行归一化处理，确保列表中的元素总和为1
        """
        tn = []
        for i in n:
            if i <= 0:
                tn.append(0)
            else:
                tn.append(i)
        s = np.sum(tn)
        if s == 0:
            for i in range(len(tn)):
                tn[i] = 1
            return [t / len(tn) for t in tn]
        new_n = [t / s for t in tn]

        return new_n



    def equal(self, a, b):
        """
        判断两个元素是否相等，相等返回-3，不相等返回3
        """
        if a == b:
            return -3
        else:
            return 3

    def sigmod(self, n):
        return 1 / (1 + np.exp(-n))

    def count_change_ratio(self, code_tokens, orig_code, identifiers_len):
        """
        计算对抗样本与原始样本之间的修改比例
        """
        change_ratio = float(np.sum(code_tokens != orig_code)) / float(identifiers_len)
        return change_ratio
    '''
    def gen_pos_saliency(self,pos,x_orig,target,orig_score):
        text_orig=[self.inv_dict[t] for t in x_orig if t!=0]
        text_new=' '.join(self.do_replace(text_orig,pos,'<unk>'))
        s_new=self.predict_text(text_new)[target]

        saliency=orig_score-s_new
        return saliency
    '''
    def attack(self, idx, example, orig_code, pos_tags=None):
        
        # 先预测
        logits, preds = self.model_tgt.get_results([example], self.args.eval_batch_size)
        orig_prob = logits[0]
        orig_label = preds[0]
        current_prob = max(orig_prob)
        if self.args.model_name == 'codebert':
            true_label = example[1].item()
        elif self.args.model_name == 'graphcodebert':
            true_label = example[3].item()
        elif self.args.model_name == 'codet5':
            true_label = example[1].item()
        query_times = 0
        flag_success = False  # whether we have found the first adversarial example in our algorithm
        is_success = 0

        # 对应代码的变量名替换
        substituions = self.substitutions[idx]
        variable_names, function_names, code_tokens = get_identifiers(orig_code, "c")
        if len(variable_names) == 0:
            return -2, None, None
        variable_names = variable_names + function_names

        # print(f"variable_names in {idx}轮: {variable_names}\n")

        processed_code = " ".join(code_tokens)

        adv_words, adv_sub_words, adv_keys = _tokenize(processed_code, self.tokenizer_mlm)

        if orig_label != true_label:  # the original code is misclassified
            logger.info("The original code is misclassified.")
            is_success = -4
            return is_success, None, None
        elif len(variable_names) == 0:  # no identifier in the code
            logger.info("No variable_names in the code.")
            is_success = -3
            return is_success, None, None

        # 获得每个变量在代码中的位置 [name1: [pos1, pos2, ...], name2: [pos1, pos2, ...], ...]
        names_positions_dict = get_identifier_posistions_from_code(
            code_tokens, variable_names
        )

        # 存储所有变量的位置信息
        pos_dict = []
        for i in variable_names:
            pos_dict.append(names_positions_dict[i])

        # 清空 invoke_dict 缓存，用于存储已经预测过的样本
        self.invoke_dict = {}

        identifiers_len = len(code_tokens)
        print("Number of variable_names extracted: ", len(variable_names))
    
        # 初始化时,每个粒子依次替换一个变量名。怎么替换呢，用fasttext计算变量和他所有替换的相似度，然后选择最相似的替换
        temp_subs_variable_name = {}
        substituions_len = 0
        # TODO substituions中变量少于variable_names
        # print(f"substituions:{substituions}")
        
        variable_names_tmp = []
        for name, subs in substituions.items():
            substituions_len = substituions_len+1
            temp_subs_variable_name[name] = []
            variable_names_tmp.append(name)
            for sub in subs:
            # temp_subs_variable_name.update(subs)
                temp_subs_variable_name[name].append([sub, self.fasttext_model.get_word_vector(sub)])

        # print(f"temp_subs_variable_name in {idx}轮:{temp_subs_variable_name}")

        if substituions_len == 0:
            print("substituions_len == 0")
            return -1, None, None

        temp_var = {}   
        for name, subs in temp_subs_variable_name.items():
            temp_var[name] =[]
            name_vec = self.fasttext_model.get_word_vector(name)
            for j in subs:
                temp_var[name].append([j[0], 1-cosine_distance(name_vec, j[1])])

            temp_var[name] = sorted(temp_var[name], key=lambda x: x[1], reverse=True)
        
        print(f"temp_var:{temp_var}") # 存储了每个变量对应余弦距离最大的替换词

        # 没有替换词
        if len(substituions) == 0:
            return  -3

        # pop_size = min(len(substituions), self.pop_size)
        # 根据temp_var初始化种群,每个粒子随机替换一个变量名
        # pop中存的是粒子群,也就是初始化后的对抗性代码
        is_success,pop,pop_scores = self.generate_population(code_tokens, substituions, temp_var, true_label,self.pop_size,current_prob,variable_names_tmp)

        # print(f"adv_codes: {pop}\n origin_code:{orig_code}\n")
        if is_success==1:
            return is_success,pop,pop_scores
        
        # else:
        #     print("先测试到这里")
        #     return is_success,pop,pop_scores
        
        # 粒子群体
        part_elites = copy.deepcopy(pop)
        # 粒子群对应的gap分数
        part_elites_scores = pop_scores
        # 最优最大的粒子的gap分数
        all_elite_score = np.max(pop_scores)
        pop_ranks = np.argsort(pop_scores)

        top_attack = pop_ranks[-1]
        # 最优对抗性代码
        all_elite = pop[top_attack]

        print(f"{idx}轮初始的最优对抗性代码,all_elite:{all_elite}\n")
        # PSO超参数
        # Omega_1 和 Omega_2:惯性权重的最大值和最小值
        Omega_1 = 0.8
        Omega_2 = 0.2
        # C1_origin 和 C2_origin:个体学习因子和社会学习因子
        C1_origin = 0.8
        C2_origin = 0.2
        # 速度初始化为随机值
        V = [np.random.uniform(-3, 3) for rrr in range(self.pop_size)]
        # print(f"初始化的速度v:{V}")
        # 速度更新,维度与变量名数目相同  V_P:[pop_size][substituions_len]
        # 这里identifiers_len是整个代码片段的长度
        V_P = [[V[t] for rrr in range(identifiers_len)] for t in range(self.pop_size)]
        # print(f"初始化的粒子速度V_P:{V_P}")
        # PSO迭代

        # return -1, None, None # TEST  
    
        min_prob = current_prob
        for i in range(self.max_iters):
            # 逐步调整速度和位置，以寻找最优对抗样本
            Omega = (Omega_1 - Omega_2) * (self.max_iters - i) / self.max_iters + Omega_2
            C1 = C1_origin - i / self.max_iters * (C1_origin - C2_origin)
            C2 = C2_origin + i / self.max_iters * (C1_origin - C2_origin)

            # 粒子更新,根据其当前速度和最优个体（粒子自身和全局最优个体）更新位置（对抗代码）
            for id in range(self.pop_size):
                # 位置有维度,每个维度代表一个变量名,逐个维度更新
                # TODO 需要确定pop[id][dim]是否是变量
                for dim in range(identifiers_len):
                    current_var_name = pop[id][dim]
                    if not current_var_name in names_positions_dict or dim not in pos_dict:
                        V_P[id][dim] = -np.inf
                        continue
                    V_P[id][dim] = Omega * V_P[id][dim] + (1 - Omega) * (self.equal(pop[id][dim], part_elites[id][dim]) + self.equal(pop[id][dim],all_elite[dim]))
                # 根据粒子的速度决定每个位置对应的变量是否进行修改
                turn_prob = [self.sigmod(V_P[id][d]) for d in range(identifiers_len)]

                # 通过调整局部搜索和全局搜索之间的平衡来提高搜索效率,在早期阶段围绕各自的最佳位置进行空间搜索，在最后阶段围绕全局最佳位置进行更好的位置搜索
                P1 = C1 # 通过P1判断粒子是否整体移动到其个体的最佳位置,一旦一个粒子决定移动，其位置的每个维度的变化取决于其速度的相同维度，特别是与sigmoid的概率有关
                P2 = C2 # 以P2来决定是否移动到全局最佳位置,每个位置维度的变化也依赖于sigmoid
                # P1=self.sigmod(P1)
                # P2=self.sigmod(P2)

                # 为了加强在未探索空间中的搜索，在更新步骤后对每个粒子进行变异。为避免过度修改，以turn_prob概率进行变异,分成两步执行
                if np.random.uniform() < P1:
                    pop[id] = self.turn(part_elites[id], pop[id], turn_prob, identifiers_len)
                if np.random.uniform() < P2:
                    pop[id] = self.turn(all_elite, pop[id], turn_prob, identifiers_len)
            # 评估
            pop_scores = []
            pop_scores_all=[]
            # for a in pop:
            #     pt = self.predict(a)

            #     pop_scores.append(pt[label])
            #     pop_scores_all.append(pt)

            # return -1, None, None #TEST

            # TODO 
            logits, preds = self.predict_batch(pop, [true_label for _ in range(self.pop_size)])
            # print(f"优化时的logits:{logits}\n preds:{preds}")
            for index, temp_prob in enumerate(logits):
                temp_label = preds[index]
                if temp_label != orig_label:
                    print("rename in evaluate %s SUCCESS! (%.5f => %.5f)" % ('>>', current_prob, temp_prob[orig_label]), flush=True)
                    return 1, pop[index], current_prob - temp_prob[orig_label]
                else:
                    pop_scores.append(current_prob-temp_prob[orig_label])

                    if min_prob > temp_prob[orig_label]:
                        min_prob = temp_prob[orig_label]

            # 从小到大排序,返回的是索引
            pop_ranks = np.argsort(pop_scores)
            top_attack = pop_ranks[-1]

            print('\t\t', i, ' -- ', pop_scores[top_attack])

            # 更新种群和最优个体
            new_pop = []
            new_pop_scores=[]
            for id in range(len(pop)):
                adv_code=pop[id]
                change_ratio = self.count_change_ratio(adv_code, code_tokens, identifiers_len)
                p_change = 1 - 2*change_ratio
                if np.random.uniform() < p_change:
                    # TODO 
                    is_success, adv_code, gap_prob = self.perturb(adv_code, code_tokens, substituions, temp_var, true_label,current_prob)
                    if is_success == 1:
                        print("rename in update %s SUCCESS! (%.5f => %.5f)" % ('>>', current_prob, temp_prob[orig_label]), flush=True)
                        return is_success, adv_code, gap_prob
                    else:
                        # TODO 待修改逻辑
                        new_pop_scores.append(gap_prob)
                        new_pop.append(adv_code)
                else:
                    new_pop_scores.append(pop_scores[id])
                    new_pop.append(adv_code)
            
            pop = new_pop

            pop_scores = new_pop_scores
            # 升序之后的索引
            pop_ranks = np.argsort(pop_scores)
            top_attack = pop_ranks[-1]
            for k in range(self.pop_size):
                if pop_scores[k] > part_elites_scores[k]:
                    part_elites[k] = pop[k]
                    part_elites_scores[k] = pop_scores[k]
            elite = pop[top_attack]
            if np.max(pop_scores) > all_elite_score:
                all_elite = elite
                all_elite_score = np.max(pop_scores)
        return -1,all_elite, all_elite_score